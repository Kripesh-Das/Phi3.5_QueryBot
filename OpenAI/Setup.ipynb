{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "open_ai_key = os.getenv(\"OPEN_AI_KEY\")\n",
    "if open_ai_key is not None:\n",
    "    os.environ[\"OPEN_API_KEY\"] = open_ai_key\n",
    "\n",
    "## Lnagsmith Tracking\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "if langchain_api_key is not None:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
    "\n",
    "langchain_project = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "if langchain_project is not None:\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = langchain_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7fd4582e77d0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fd44b8c7890> root_client=<openai.OpenAI object at 0x7fd4588b5b50> root_async_client=<openai.AsyncOpenAI object at 0x7fd44bad5890> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Hunter x Hunter\" is a Japanese manga series written and illustrated by Yoshihiro Togashi. It was first serialized in Weekly Shōnen Jump magazine in March 1998. The story follows a young boy named Gon Freecss, who discovers that his father, who he thought was dead, is actually a legendary \"Hunter\"—a specialist who ventures into dangerous areas, hunts for rare treasures, and takes on various other challenges. Inspired by his father\\'s legacy, Gon decides to become a Hunter himself and sets off on a journey to pass the rigorous Hunter Examination and find his father.\\n\\nThe series is well-known for its intricate plot, well-developed characters, and unique power system called \"Nen,\" which allows characters to use and manipulate their own life energy in various ways. \"Hunter x Hunter\" has been adapted into two anime television series, with the most recent one airing from 2011 to 2014. Despite its popularity, the manga has experienced numerous hiatuses due to Togashi\\'s health issues, leading to an irregular publication schedule.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 210, 'prompt_tokens': 13, 'total_tokens': 223, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2f406b9113', 'finish_reason': 'stop', 'logprobs': None}, id='run-83be25fd-cc8d-4227-87e3-98ca64d6380b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 210, 'total_tokens': 223, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## INPUT AND RESPONSE\n",
    "llm.invoke(\"What is HunterxHunter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHATPROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are expert AI Engineer. Provide answers to all my queries'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are expert AI Engineer. Provide answers to all my queries\"),\n",
    "        (\"user\",\"{input}\")  \n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LLaMA 2, which stands for \"Large Language Model Meta AI,\" is a series of open-weight language models released by Meta (formerly Facebook) in July 2023. LLaMA 2 is an evolution of the original LLaMA models and is designed to improve on both performance and accessibility in the realm of natural language processing (NLP).\\n\\nHere are some key features and details about LLaMA 2:\\n\\n1. **Model Variants**: LLaMA 2 comes in various sizes, typically including models with billions of parameters, allowing them to handle a wide range of NLP tasks with improved accuracy and understanding.\\n\\n2. **Open Access**: Unlike some proprietary models, LLaMA 2\\'s weights are openly available, enabling researchers and developers to use, modify, and build upon the models without restrictive licensing. This openness encourages collaboration and innovation within the AI community.\\n\\n3. **Training**: LLaMA 2 models were trained on a mix of publicly available datasets, which helps in enhancing their ability to generate coherent and contextually relevant text.\\n\\n4. **Applications**: Like other large language models, LLaMA 2 can be applied to a variety of tasks, including text generation, translation, summarization, question-answering, and more.\\n\\n5. **Performance**: LLaMA 2 has shown competitive performance compared to other leading language models, often excelling in benchmarks that test understanding and generation of human-like text.\\n\\n6. **Ethical Considerations**: Meta has addressed ethical considerations by providing guidelines and tools to reduce biases and undesirable outputs, though challenges in this area persist, as with all AI models.\\n\\n7. **Research and Development**: The open nature of LLaMA 2 supports a wide array of research initiatives, enabling academic and corporate entities to explore novel approaches to NLP and contribute back to the community.\\n\\nFor developers and researchers interested in using LLaMA 2, it\\'s important to consider the computational resources required, as larger models can be resource-intensive. However, their availability provides a valuable tool for advancing AI applications in various domains.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 425, 'prompt_tokens': 31, 'total_tokens': 456, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6b68a8204b', 'finish_reason': 'stop', 'logprobs': None} id='run-250a9721-16d1-447f-9bd0-637fb1d05d7a-0' usage_metadata={'input_tokens': 31, 'output_tokens': 425, 'total_tokens': 456, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "response = chain.invoke({\"input\":\"Can you tell me about llama 2\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STR OUTPUT PARSER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LLaMA 2, which stands for \"Large Language Model Meta AI,\" is a series of open-source language models developed by Meta (formerly Facebook). Released in July 2023, LLaMA 2 is an evolution of the original LLaMA models, designed to be more powerful and efficient for a variety of natural language processing tasks.\\n\\nKey features of LLaMA 2 include:\\n\\n1. **Model Sizes**: LLaMA 2 is available in multiple sizes, typically ranging from 7 billion to 70 billion parameters. This scaling allows for flexibility in deployment, accommodating different resource availability and performance requirements.\\n\\n2. **Training and Architecture**: LLaMA 2 models are trained on a diverse range of internet text data, enabling them to understand and generate human-like text. The training process involves a mixture of supervised learning and reinforcement learning from human feedback (RLHF) to improve model alignment and safety.\\n\\n3. **Open-source**: One of the significant aspects of LLaMA 2 is its open-source nature, allowing researchers and developers to access the models and adapt them for various applications. This fosters innovation and ensures transparency in how the models are used and developed.\\n\\n4. **Performance**: LLaMA 2 models have demonstrated competitive performance on numerous language benchmarks, often rivaling or exceeding the capabilities of other state-of-the-art models from different organizations.\\n\\n5. **Applications**: The LLaMA 2 models can be used for a wide array of applications, including chatbots, content generation, translation, summarization, and more, owing to their versatility and understanding of context in language.\\n\\nThe release of LLaMA 2 represents Meta\\'s commitment to advancing AI research and making powerful AI tools accessible to the broader community. However, as with any AI model, considerations around ethical use, bias, and safety are important to address in practical applications.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 381, 'prompt_tokens': 31, 'total_tokens': 412, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e5e4913e83', 'finish_reason': 'stop', 'logprobs': None} id='run-05fb1346-69e6-4372-974a-e774228ab430-0' usage_metadata={'input_tokens': 31, 'output_tokens': 381, 'total_tokens': 412, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "chain = prompt|llm\n",
    "response = chain.invoke({\"input\":\"Can you tell me about llama 2\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
